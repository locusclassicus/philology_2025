# Лемматизация и тематическое моделирование

Основные этапы NLP включают в себя токенизацию, морфологический и синтаксический анализ, а также анализ семантики и прагматики. В этом уроке речь пойдет про первые три этапа. Мы научимся разбивать текст на токены (слова), определять морфологические характеристики слов и находить их начальные формы (леммы), а также анализировать структуру предложения с использованием синтаксических парсеров. 

```{r message=FALSE}
library(tidyverse)
library(tidytext)
library(udpipe)
```


## Данные

За основу для всех эти вычислений мы возьмем три философских трактата, написанных на английском языке. Это хронологически и тематически близкие тексты:

- "Опыт о человеческом разумении" Джона Локка (1690), первые две книги;
- "Трактат о принципах человеческого знания" Джорджа Беркли  (1710);
- "Исследование о человеческом разумении" Дэвида Юма (1748).

Данные были извлечены из открытой библиотеки [Gutengerg](https://www.gutenberg.org/) при помощи [пакета](https://docs.ropensci.org/gutenbergr/) `{gutenbergr}`и приведены к опрятному виду. Скачать подготовленный таким образом корпус можно [здесь](https://github.com/locusclassicus/philology_2025/raw/refs/heads/main/data/emp_corpus.Rdata).

```{r}
load("../data/emp_corpus.Rdata")
```


Делим корпус на слова уже известным вам способом.

```{r}
corpus_words <- emp_corpus |> 
  unnest_tokens(word, text)

corpus_words
```

Большая часть слов, которые мы сейчас видим в корпусе  -- это шумовые слова, или стоп-слова, не несущие смысловой нагрузки. 

```{r}
author_word_counts <- corpus_words  |> 
  count(author, word, sort = T) 

author_word_counts |> 
  slice_head(n = 15) |> 
  ggplot(aes(reorder(word, n), n, fill = word)) +
  geom_col(show.legend = F) + 
  facet_wrap(~author, scales = "free") +
  coord_flip() +
  theme_light()
```

Обратите внимание: абсолютная частотность -- плохой показатель для текстов разной длины. Чтобы тексты было проще сравнивать, следует разделить показатели частотности на общее число токенов в тексте. 


## Наиболее характерные слова

Наиболее частотные слова наименее подвержены влиянию тематики, поэтому их используют для стилометрического анализа. Если отобрать наиболее частотные после удаления стоп-слов, то мы получим достаточно адекватное отражение _тематики_ документов. 

Если же мы необходимо найти наиболее _характерные_ для документов токены, то применяется другая мера, которая называется _tf-idf_ (term frequency - inverse document frequency). 

![](./images/tf_idf.png) 


Логарифм единицы равен нулю, поэтому если слово встречается во всех документах, его tf-idf равно нулю. Чем выше tf-idf, тем более характерно некое слово для документа. При этом относительная частотность тоже учитывается! Например, Беркли один раз упоминает "сахарные бобы", а Локк -- "миндаль", но из-за редкой частотности tf-idf для подобных слов будет низкой.

Функция `bind_tf_idf()` принимает на входе тиббл с абсолютной частотностью для каждого слова. 

```{r}
author_word_tfidf <- author_word_counts |> 
  bind_tf_idf(word, author, n)

author_word_tfidf
```

Вот так выглядят наиболее характерные слова для каждого автора

```{r message=FALSE}
author_word_tfidf |> 
  group_by(author) |>
  arrange(-tf_idf) |> 
  top_n(15) |> 
  ungroup() |> 
  ggplot(aes(reorder_within(word, tf_idf, author), tf_idf, fill = author)) +
  geom_col(show.legend = F) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~author, scales = "free") +
  scale_x_reordered() +
  coord_flip()
```

На такой диаграмме авторы совсем не похожи друг на друга, но будьте осторожны: все то, что их сближает (а это не только служебные части речи!), сюда просто не попало. Можно также заметить, что ряд характерных слов связаны не столько с тематикой, сколько со стилем: чтобы этого избежать, можно использовать лемматизацию или задать правило для замены вручную.

## Лемматизация и POS-тэггинг

Лемматизация -- приведение слов к начальной форме (лемме). Как правило, она проводится одновременно с частеречной разметкой (POS-tagging). Все это умеет делать [UDPipe](https://lindat.mff.cuni.cz/services/udpipe/run.php) -- обучаемый конвейер (trainable pipeline), для которого существует одноименный [пакет](https://rdrr.io/cran/udpipe/) в R. 

Основным форматом файла для него является CoNLL-U.  Файлы в таком формате хранятся в так называемых трибанках, то есть коллекциях уже размеченных текстов (название объясняется тем, что синтаксическая структура предложений представлена в них в виде древовидных графов). Файлы CoNLL-U используются для обучения нейросетей, но для большинства языков доступны хорошие предобученные модели, работать с которыми достаточно просто. 

Пакет `udpipe` позволяет работать со множеством языков (всего 65), для многих из которых представлено несколько моделей, обученных на разных трибанках. Прежде всего нужно выбрать и загрузить модель   ([список](https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html)). Описания моделей доступны на сайте <https://universaldependencies.org/>.


```{r eval=FALSE}
# скачиваем модель в рабочую директорию
#udpipe_download_model(language = "english-gum")

# загружаем модель
english_gum <- udpipe_load_model(file = "english-gum-ud-2.5-191206.udpipe")

# аннотируем (это займет несколько минут)
emp_ann <- udpipe_annotate(english_gum, emp_corpus$text, doc_id = emp_corpus$author)
```

Результат возвращается в формате [CoNLL-U](https://universaldependencies.org/format.html); это широко применяемый формат представления результат морфологического и синтаксического анализа текстов. 

Вот пример разбора предложения:

![](https://www.researchgate.net/publication/341522061/figure/fig1/AS:893293068046336@1589989072121/1-CONLL-U-format-example.ppm)

Cтроки слов содержат следующие поля:

1. `ID`: индекс слова, целое число, начиная с 1 для каждого нового
предложения; может быть диапазоном токенов с несколькими словами.
2. `FORM`: словоформа или знак препинания.
3. `LEMMA`: Лемма или основа словоформы.
4. `UPOSTAG`: тег части речи из универсального набора проекта UD, который создавался для того, чтобы аннотации разных языков были сравнимы между собой.
5. `XPOSTAG`: тег части речи, который выбрали исследователи под конкретные нужды языка
6. `FEATS`: список морфологических характеристик.
7. `HEAD`: идентификатор (номер) синтаксической вершины текущего токена. Если такой вершины нет, то ставят ноль (0).
8. `DEPREL`: характер синтаксической зависимости.
9. `DEPS`: Список вторичных зависимостей.
10. `MISC`: любая другая аннотация.

Для работы данные удобнее трансформировать в прямоугольный формат.

```{r eval=FALSE}
emp_pos <- as_tibble(emp_ann) |> 
  select(-paragraph_id)

emp_pos 
```

```{r echo=FALSE, eval=FALSE}
save(emp_pos, file = "../data/emp_pos.Rdata")
```

```{r echo=FALSE}
load("../data/emp_pos.Rdata")
emp_pos 
```


### Поля UPOS и XPOS

Морфологическая аннотация, которую мы получили, дает возможность выбирать и группировать различные части речи. Например, существительные.

```{r}
emp_pos |> 
  filter(upos == "NOUN") |> 
  select(doc_id, token, lemma, upos, xpos)
```

Посчитать [части речи](https://universaldependencies.org/u/pos/) можно так:

```{r}
upos_counts <- emp_pos |>
  count(doc_id, upos, sort = TRUE) 

upos_counts
```

Относительные значения:

```{r message=FALSE}
total_counts <- upos_counts |> 
 count(doc_id, wt = n, name = "sum_n")

pos_share <- upos_counts |> 
  left_join(total_counts) |> 
  mutate(share = round( (n/sum_n), 2))

pos_share |> 
  ggplot(aes(reorder(upos, share), share, fill = upos)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  theme_light() +
  facet_wrap(~doc_id, scales = "free")
```

Отберем наиболее частотные имена существительные и имена собственные.

```{r}
nouns <- emp_pos  |> 
  filter(upos %in% c("NOUN", "PROPN")) |> 
  count(doc_id, lemma, sort = TRUE) 

nouns
```

```{r}
nouns |> 
  group_by(doc_id) |> 
  slice_head(n = 10) |> 
  ggplot(aes(reorder_within(lemma, n, doc_id), n, fill = lemma)) +
  geom_col(show.legend = FALSE) +
  theme_light() +
  coord_flip() +
  scale_x_reordered() +
  facet_wrap(~ doc_id, scales = "free") +
  xlab(NULL)
```

Сравните с результатом, который вы получили, считая TF-IDF.

В отличие от UPOS (Universal Part-Of-Speech), XPOS (Language-specific Part-Of-Speech) -- это теги частей речи, используемые в национальных корпусах. Форматы тегов и их детализация могут значительно меняться от языка к языку.


### Поля FEATS и DEP_REL

Допустим, нам нужны лишь определенные формы: например, прилагательные в превосходной степени.

```{r}
superlatives <- emp_pos  |> 
  filter(str_detect(feats, "Degree=Sup") & upos == "ADJ") |> 
  select(doc_id, token)

superlatives_counts <- superlatives |> 
  count(doc_id, token, sort = TRUE) |> 
  group_by(doc_id) |> 
  slice_head(n = 15)
```

```{r}
superlatives_counts |> 
  ggplot(aes(reorder_within(token, n, doc_id), n, fill = doc_id)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  theme_light() +
  facet_wrap(~ doc_id, scales = "free") +
  scale_x_reordered() +
  xlab(NULL)
```


Аналогичным образом можно отбирать синтаксические признаки (DEP_REL) и их комбинации, а также визуализировать деревья зависимостей для отдельных предложений при помощи пакета `{textplot}`.

## Совместная встречаемость слов

Функция `cooccurence()` из пакета `udpipe` позволяет 
выяснить, сколько раз некий термин встречается совместно с другим термином, например:

- слова встречаются в одном и том же документе/предложении/параграфе;
- слова следуют за другим словом;
- слова находятся по соседству с другим словом на расстоянии n слов. 

Выясним, какие существительные чаще встречаются в одном предложении у Локка:


```{r warning=FALSE}
locke_nouns <-  emp_pos |> 
  filter(doc_id == "Locke") |> 
  filter(upos == "NOUN")

locke_nouns
```

```{r}
cooc <- cooccurrence(locke_nouns, term = "lemma", 
                     group = "sentence_id") |>
  as_tibble() |> 
  filter(cooc > 70) |> 
  filter(term1 != "idea", term2 != "idea")

cooc
```

Этот результат легко визуализировать, используя пакет `ggraph`:

```{r message=FALSE, warning=FALSE, fig.width=9}
library(igraph)
library(ggraph)

wordnetwork <- graph_from_data_frame(cooc)
```

```{r message=FALSE}
ggraph(wordnetwork, layout = "fr") +
  geom_edge_arc(aes(width = cooc), alpha = 0.8, edge_colour = "grey90", show.legend=FALSE) +
  geom_node_label(aes(label = name), col = "#1f78b4", size = 4) +
  theme_void() +
  labs(title = "Совместная встречаемость существительных в предложении")
```

Чтобы узнать, какие слова чаще стоят рядом, используем ту же функцию, но [с другими аргументами](https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-usecase-postagging-lemmatisation.html):

```{r warning=FALSE}
cooc2 <- cooccurrence(locke_nouns$lemma, 
                      relevant = locke_nouns$upos %in% "NOUN", 
                      skipgram = 1) |> 
  as_tibble() |> 
  filter(cooc > 10)

cooc2
```

```{r warning=FALSE}
wordnetwork <- graph_from_data_frame(cooc2)

ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc), edge_colour = "grey90", edge_alpha=0.8, show.legend = F) +
  geom_node_label(aes(label = name), col = "#1f78b4") +
  labs(title = "Слова, стоящие рядом в тексте") +
  theme_void()
```


## Тематическое моделирование

Вы можете легко преобразовать аннотированный тибл в матрицу "документ-термин" (document-term-matrix), которая используется во многих других пакетах для анализа текста. В данном случае мы будем выделять темы (topics) на уровне предложений.

Преимущество `{udpipe}` по сравнению с другими пакетами заключается в следующем:

- [тематическое моделирование](https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-usecase-topicmodelling.html#udpipe-for-topic-modelling) (topic modelling) можно проводить непосредственно по нужным словам, так как их легко определить с помощью тегов частей речи: чаще всего интересуют только существительные и глаголы или, например, только прилагательные, если вы хотите кластеризовать текст по тональности (sentiment clustering);
- больше не нужно создавать длинные списки стоп-слов;
- кроме того, можно работать с леммами, а не с исходными формами слов, что позволяет схожим словам лучше группироваться друг с другом;
- вы легко можете включать составные ключевые слова.

Отбираем только существительные.

```{r}
emp_pos$topic_level_id <- unique_identifier(emp_pos, fields = c("doc_id", "sentence_id"))

dtf <- emp_pos |> 
  filter(upos == "NOUN")
```

```{r}
dtf <- document_term_frequencies(dtf, document = "topic_level_id", term = "lemma")
head(dtf)
```

```{r}
dtm <- document_term_matrix(x = dtf)
```

```{r}
dtm_clean <- dtm_remove_lowfreq(dtm, minfreq = 5)
sample(dtm_colsums(dtm_clean), 10)
```

```{r}
library(topicmodels)
m <- LDA(dtm_clean, k = 4, method = "Gibbs", 
         control = list(nstart = 5, burnin = 2000, best = TRUE, seed = 1:5))
```


Чтобы интерпретировать тематическую модель и подобрать подходящие названия темам, вы можете получить наиболее часто встречающиеся термины для каждой темы, указав минимальную вероятность появления термина и минимальное количество отображаемых терминов.  

```{r}
topicterminology <- predict(m, type = "terms", 
                            min_posterior = 0.005, 
                            min_terms = 3)
topicterminology
```

Кроме того, мы также можем легко использовать модель для предсказания, к какой теме принадлежит новый документ. Функция predict корректно возвращает значение NA для тех документов, в которых отсутствуют слова, использованные в модели, что обычно вызывает затруднения в других пакетах R. Также функция позволяет присваивать каждой теме метки и показывает разницу в вероятности между наиболее вероятной темой и следующей по вероятности, что полезно для оценки чёткости (разграниченности) ваших тем. 

```{r}
scores <- predict(m, newdata = dtm_clean, type = "topics", 
                  labels = c("physics", "psychology", "ethics", "epistemology"))
```

Аргумент `labels` присваивает пользовательские имена темам в итоговом результате предсказания.

Визуализируем топики. 

```{r}
emp_topics <- merge(emp_pos, scores, by.x="topic_level_id", by.y="doc_id")
```

```{r}
wordnetwork <- subset(emp_topics, topic %in% 1 & lemma %in% topicterminology[[1]]$term)
```

```{r}
wordnetwork <- cooccurrence(wordnetwork, group = c("topic_level_id"), term = "lemma")
wordnetwork <- graph_from_data_frame(wordnetwork)
```

```{r}
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "pink")  +
  geom_node_text(aes(label = name), col = "darkgreen", size = 4) +
  theme_graph(base_family = "Arial Narrow") +
  labs(title = "Words in topic Physics ", subtitle = "Nouns Cooccurrence")
```

## Видео 

- [Видео](https://vk.com/video91786643_456239085) 2025 г.

## Домашнее задание

По [ссылке](https://github.com/locusclassicus/philology_2025/raw/refs/heads/main/data/tales_text.Rdata) вы найдете датасет со сказками Салтыкова-Щедрина (в формате `.Rdata`). Вам необходимо аннотировать сказки, используя модель SynTagRus. 

После этого ответьте на вопросы по [ссылке](https://forms.gle/zb5JRk25685CHb849). Задание считается выполненным, если верные ответы даны на 3 из 5 вопросов. 

Ошибки лемматизации и морфологического анализа игнорируйте. 

Дедлайн: 28 ноября, 21-00.